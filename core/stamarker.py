import logging
import copy
import sys

sys.path.append('/home/ubuntu/clustering-framework/stamarker/')
from STAMarker.stamarker import pipeline, dataset, utils
# import dataset
# import utils

import pandas as pd
import upsetplot

from sklearn.cluster import spectral_clustering
from core import ClusteringAlgorithm
from .utils import timeit

class StamarkerAlgo(ClusteringAlgorithm):
    def __init__(self, adata, **params):
        super().__init__(adata, **params)
        self.filename = self.adata.uns['sample_name'] + f"_stamarker_nhvgs{self.stamarker__n_top_genes}_mincl{self.stamarker__min_cells}_mct{self.stamarker__min_counts}_rdcof{self.stamarker__radial_cutoff}_nae{self.stamarker__n_auto_enc}_clm{self.stamarker__clustering_method}_nclss{self.stamarker__n_clusters}_r{self.resolution}_al{self.stamarker__alpha}"

        self.cluster_key = 'stamarker'

    @timeit
    def run(self):
        # create STAMarker data_module from adata
        data_module = pipeline.make_spatial_data(self.adata)
        # preprocess data, filter genes and cells,
        # calculate highly variable genes, and normalize
        # 'seurat_v3' is default flavour for HVGs
        # target_sum =1e4 by default for normalization and log1p
        data_module.prepare_data(n_top_genes=self.stamarker__n_top_genes, rad_cutoff = self.stamarker__radial_cutoff, show_net_stats=False, min_cells=self.stamarker__min_cells, min_counts=self.stamarker__min_counts)
        
        # read model and trainer parameters
        config = dict()
        config.update(utils.parse_args("_params/model.yaml"))
        config.update(utils.parse_args("_params/trainer.yaml"))
        # update parameters if GPU is not available
        if not torch.cuda.is_available():
            config["stagate_trainer"]["gpus"] = 0
            config["stagate_trainer"]["auto_select_gpus"] = False
            config["classifier_trainer"]["gpus"] = 0
            config["classifier_trainer"]["auto_select_gpus"] = False
        
        # stamarker pipeline
        logging.info(r"Starting STAMarker pipeline")
        # initialize the `STAMarker` object
        stm = pipeline.STAMarker(n=self.stamarker__n_auto_enc,save_dir=self.adata.uns['algo_params']['out_path'], config=config, logging_level=logging.INFO)        
        logging.info(r"STAMarker object created")

        # Train autoencoders
        stm.train_auto_encoders(data_module=data_module)
        logging.info(f'{self.stamarker__n_auto_enc} auto-encoders finished training')

        # Perform Louvain or mclust clustering for each embedding
        # generated by auto-encoders
        # For Louvain cluster_params is resolution
        # For mclust cluster_params is number of clusters
        if self.stamarker__clustering_method == "louvain":
            stm.clustering(data_module=data_module, cluster_method="louvain", cluster_params=self.resolution)
        elif self.stamarker__clustering_method == "mclust":
            stm.clustering(data_module=data_module, cluster_method="mclust", cluster_params=self.stamarker__n_clusters)
        logging.info(f'Clustering using {self.stamarker__clustering_method} method done')
        
        # Consensus clustering
        # file with labels info is hardcoded
        stm.consensus_clustering(n_clusters=self.stamarker__n_clusters, name="cluster_labels.npy")
        consensus_labels = np.load(stamarker.save_dir + "/consensus_labels.npy")
        self.adata.obs["Consensus clustering"] = consensus_labels.astype(str)
        logging.info(f'Consensus clustering for {self._stamarker__n_clusters} clusters created {consensus_labels.size} clusters')

        # Train MLPs
        # file with labels info is hardcoded
        stm.train_classifiers(data_module=data_module, n_clusters=self.stamarker__n_clusters, name="cluster_labels.npy")
        
        # computer smaps - SVGs
        smap = stm.compute_smaps(data_module=data_module, return_recon=True, normalize=True)
        
        # select SVGs
        domain_svg_list = []
        for domain_ind in range(self.stamarker__n_clusters):
            domain_svg_list.append(utils.select_svgs(np.log(1 + smap), domain_ind, consensus_labels, alpha=self.stamarker__alpha))


        # save SVGs, their p_val_adj and modules to adata.uns
        self.adata.uns['svg_'+self.cluster] = domain_svg_list
        logging.info(f'STAMarker finished identifying spatially variable genes and their modules. Added results to adata.uns["svg_{self.cluster}"]')

        ## save gene modules to adata.uns
        #self.adata.uns['svg_modules_'+self.cluster] = hs.modules
        #logging.info(f'STAMarker finished identifying spatially variable gene modules. Added results to adata.uns["svg_modules_{self.cluster}"]')

        #self.adata.obsm['embedding'] = hs.module_scores
        #logging.info(r"Module scores saved in self.adata.obsm['embedding']")

        if self.svg_only:
            return

        # [NOTE] add clustering based on module scores embeddings

    def save_results(self):
        self.adata.write(f'{self.filename}.h5ad', compression="gzip")
        logging.info(f'Saved clustering result {self.filename}.h5ad')
        
        upset_domains_df = upsetplot.from_contents({ f"Spatial domain {ind}": l for ind, l in enumerate(self.data.uns['svg_'+self.cluster])})
        fig, ax = plt.subplots(1, 1, figsize=(2.7, 2.5))
        df = pd.DataFrame(upset_domains_df.index.to_frame().apply(np.sum, axis=1))
        df.columns = ["counts"]
        df.index = upset_domains_df["id"]
        df_counts = df.groupby("counts")["counts"].agg("count")
        ax.bar(df_counts.index, df_counts.values)
        ax.set_xticks(df_counts.index)
        ax.set_xlabel("Number of spatial domains")
        ax.set_ylabel("Number of genes")
        fig.savefig(f'{self_filename}_upsetplot.png')
        
        
        self.adata.uns['svg'+self.cluster].to_csv(f'{self.filename}_svg.csv', index=True)
        #self.adata.uns['svg_modules'].to_csv(f'{self.filename}_svg_modules.csv', index=True)